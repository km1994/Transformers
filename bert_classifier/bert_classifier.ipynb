{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEiJlTXwVzdO"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "# 基于 Bert 做中文文本分类\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8HDqtnYWA_l"
   },
   "source": [
    "## 连接 Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Ve33Ue2tWBSV"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ZbWC4-6pWIRd"
   },
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/NLP_study/transformers_wp/bert_classifier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "nZ6l15ejWXlN"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "fdXDoMKrWh7t"
   },
   "outputs": [],
   "source": [
    "!pip install urllib3==1.25.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "aTsJmSiRWaLt"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h5CmRg0DV48t"
   },
   "source": [
    "## 加载库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "-2B2mFW9VtDg"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.optim import optimizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from torch.nn import CrossEntropyLoss,BCEWithLogitsLoss\n",
    "from tqdm import tqdm_notebook, trange\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "from sklearn.metrics import precision_recall_curve,classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KoUO7-TuK1sh"
   },
   "source": [
    "## 工具类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "e8HNoLACTsDP"
   },
   "outputs": [],
   "source": [
    "# 功能：字典数据 存储 为 plk\n",
    "def save_plk_dict(dic,save_path,fila_name):\n",
    "    '''\n",
    "        功能：字典数据 存储 为 plk\n",
    "        input:\n",
    "        dic          Dict     存储字典    \n",
    "        save_path     String    存储目录 \n",
    "        fila_name     String    存储文件 \n",
    "    return:\n",
    "        \n",
    "    '''\n",
    "    with open(save_path+ fila_name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(dic, f, pickle.HIGHEST_PROTOCOL) \n",
    "\n",
    "def load_plk_dict(save_path,fila_name):\n",
    "    '''\n",
    "        功能：加载 plk 中 字典数据\n",
    "        input: \n",
    "        save_path     String    存储目录 \n",
    "        fila_name     String    存储文件 \n",
    "    return:\n",
    "        dic        Dict     字典数据 \n",
    "        \n",
    "    '''\n",
    "    with open(save_path+ fila_name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXrO8vjIW1L0"
   },
   "source": [
    "## 参数初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "m_ch5iYYWsj9"
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "  def __init__(self):\n",
    "    self.split_ratio = 0.9   #训练和验证集的比例\n",
    "    #MAX_SEQ_LEN = 50\n",
    "    self.batch_size = 128\n",
    "    self.seed = 0\n",
    "    self.epochs = 2\n",
    "    self.num_labels = 10\n",
    "    self.is_colab = True\n",
    "    if self.is_colab:\n",
    "      self.data_path = \"data/\"\n",
    "      self.bert_model_path = \".././../../bert_series/bert/bert-chinese-wwm_torch/\"\n",
    "    else:\n",
    "      self.data_path = \"F:/document/datasets/nlpData/text_classifier_data/THUCNews_ch/\"\n",
    "      self.bert_model_path = \"F:/document/datasets/nlpData/pretrain/bert/chinese_wwm_ext_pytorch/\"\n",
    "\n",
    "    self.data_file_list = [\n",
    "        \"cnews.train\",\"cnews.val\",\"cnews.test\"\n",
    "    ]\n",
    "\n",
    "    self.data_index = 2\n",
    "    self.patience = 20\n",
    "    self.do_lower_case = False\n",
    "    self.device = torch.device('cpu' if not torch.cuda.is_available() else \"cuda\")\n",
    "    self.is_demo = True\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTJKb1KmX73l"
   },
   "source": [
    "## 早停法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "-g6Y1kThW4_9"
   },
   "outputs": [],
   "source": [
    "#早停法\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        # torch.save(model.state_dict(), 'checkpoint_loss.pt')\n",
    "        torch.save(model, open(\"checkpoint_loss.bin\", \"wb\"))\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtBOnYqTYCiO"
   },
   "source": [
    "## Label Smoothing（标签平滑法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "aXIT2dngYDAG"
   },
   "outputs": [],
   "source": [
    "#标签平滑\n",
    "class LabelSmoothing(nn.Module):\n",
    "    def __init__(self, size, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        #self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing#if i=y的公式\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        x表示输入 (N，M)N个样本，M表示总类数，每一个类的概率log P\n",
    "        target表示label（M，）\n",
    "        \"\"\"\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()#先深复制过来\n",
    "        #print true_dist\n",
    "        true_dist.fill_(self.smoothing / (self.size - 1))#otherwise的公式\n",
    "        #print true_dist\n",
    "        #变成one-hot编码，1表示按列填充，\n",
    "        #target.data.unsqueeze(1)表示索引,confidence表示填充的数字\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        self.true_dist = true_dist\n",
    "\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBg6jYV-YSGe"
   },
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XV9oW_TjYb4P"
   },
   "source": [
    "### 数据处理类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "IjQG3DJcYSet"
   },
   "outputs": [],
   "source": [
    "# 数据处理类\n",
    "class DataPrecessForSingleSentence(object):\n",
    "    \"\"\"\n",
    "    对文本进行处理\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert_tokenizer, max_workers=10):\n",
    "        \"\"\"\n",
    "        bert_tokenizer :分词器\n",
    "        dataset        :包含列名为'text'与'label'的pandas dataframe\n",
    "        \"\"\"\n",
    "        self.bert_tokenizer = bert_tokenizer\n",
    "        # 创建多线程池\n",
    "        self.pool = ThreadPoolExecutor(max_workers=max_workers)\n",
    "        # 获取文本与标签\n",
    "\n",
    "    def get_input(self, dataset, max_seq_len=50):\n",
    "        \"\"\"\n",
    "            通过多线程（因为notebook中多进程使用存在一些问题）的方式对输入文本进行分词、ID化、截断、填充等流程得到最终的可用于模型输入的序列。\n",
    "\n",
    "            入参:\n",
    "                dataset     : pandas的dataframe格式，包含两列，第一列为文本，第二列为标签。标签取值为{0,1}，其中0表示负样本，1代表正样本。\n",
    "                max_seq_len : 目标序列长度，该值需要预先对文本长度进行分别得到，可以设置为小于等于512（BERT的最长文本序列长度为512）的整数。\n",
    "\n",
    "            出参:\n",
    "                seq         : 在入参seq的头尾分别拼接了'CLS'与'SEP'符号，如果长度仍小于max_seq_len，则使用0在尾部进行了填充。\n",
    "                seq_mask    : 只包含0、1且长度等于seq的序列，用于表征seq中的符号是否是有意义的，如果seq序列对应位上为填充符号，\n",
    "                              那么取值为1，否则为0。\n",
    "                seq_segment : shape等于seq，因为是单句，所以取值都为0。\n",
    "                labels      : 标签取值为{0,1}，其中0表示负样本，1代表正样本。   \n",
    "        \"\"\"\n",
    "        sentences = dataset.iloc[:, 0].tolist()\n",
    "        labels = dataset.iloc[:, 1].tolist()\n",
    "        # 切词\n",
    "        tokens_seq = list(\n",
    "            self.pool.map(self.bert_tokenizer.tokenize, sentences))\n",
    "        # 获取定长序列及其mask\n",
    "        result = list(\n",
    "            self.pool.map(self.trunate_and_pad, tokens_seq,\n",
    "                          [max_seq_len] * len(tokens_seq)))\n",
    "        seqs = [i[0] for i in result]\n",
    "        seq_masks = [i[1] for i in result]\n",
    "        seq_segments = [i[2] for i in result]\n",
    "        return seqs, seq_masks, seq_segments, labels\n",
    "\n",
    "    def trunate_and_pad(self, seq, max_seq_len):\n",
    "        \"\"\"\n",
    "            1. 因为本类处理的是单句序列，按照BERT中的序列处理方式，需要在输入序列头尾分别拼接特殊字符'CLS'与'SEP'，\n",
    "               因此不包含两个特殊字符的序列长度应该小于等于max_seq_len-2，如果序列长度大于该值需要那么进行截断。\n",
    "            2. 对输入的序列 最终形成['CLS',seq,'SEP']的序列，该序列的长度如果小于max_seq_len，那么使用0进行填充。\n",
    "\n",
    "            入参: \n",
    "                seq         : 输入序列，在本处其为单个句子。\n",
    "                max_seq_len : 拼接'CLS'与'SEP'这两个特殊字符后的序列长度\n",
    "\n",
    "            出参:\n",
    "                seq         : 在入参seq的头尾分别拼接了'CLS'与'SEP'符号，如果长度仍小于max_seq_len，则使用0在尾部进行了填充。\n",
    "                seq_mask    : 只包含0、1且长度等于seq的序列，用于表征seq中的符号是否是有意义的，如果seq序列对应位上为填充符号，\n",
    "                              那么取值为1，否则为0。\n",
    "                seq_segment : shape等于seq，因为是单句，所以取值都为0。\n",
    "           \n",
    "        \"\"\"\n",
    "        # 对超长序列进行截断\n",
    "        if len(seq) > (max_seq_len - 2):\n",
    "            seq = seq[0:(max_seq_len - 2)]\n",
    "        # 分别在首尾拼接特殊符号\n",
    "        seq = ['[CLS]'] + seq + ['[SEP]']\n",
    "        # ID化\n",
    "        seq = self.bert_tokenizer.convert_tokens_to_ids(seq)\n",
    "        # 根据max_seq_len与seq的长度产生填充序列\n",
    "        padding = [0] * (max_seq_len - len(seq))\n",
    "        # 创建seq_mask\n",
    "        seq_mask = [1] * len(seq) + padding\n",
    "        # 创建seq_segment\n",
    "        seq_segment = [0] * len(seq) + padding\n",
    "        # 对seq拼接填充序列\n",
    "        seq += padding\n",
    "        assert len(seq) == max_seq_len\n",
    "        assert len(seq_mask) == max_seq_len\n",
    "        assert len(seq_segment) == max_seq_len\n",
    "        return seq, seq_mask, seq_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-DHFr_-60IdF"
   },
   "source": [
    "### 数据处理函数定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "ljDl9q0pz4MS"
   },
   "outputs": [],
   "source": [
    "# 功能：数据加载函数\n",
    "def load_data(data_path,data_file):\n",
    "  '''\n",
    "    功能：数据加载函数\n",
    "    input:\n",
    "      data_path     String      数据目录   \n",
    "      data_file     String      数据文件名称\n",
    "    return:\n",
    "      data        List       数据\n",
    "      num_labels     int       标签类别数量\n",
    "  '''\n",
    "  # 数据加载\n",
    "  data = pd.read_table(f'{data_path}{data_file}.txt', encoding='utf-8', names=['label', 'text'])\n",
    "  data = data[['text', 'label']]\n",
    "  # 标签编码\n",
    "  le = LabelEncoder()\n",
    "  le.fit(data.label.tolist())\n",
    "  data['label_id'] = le.transform(data.label.tolist())\n",
    "  return data[['text', 'label', 'label_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dXJJF6GC1Hej"
   },
   "outputs": [],
   "source": [
    "train_data = load_data(config.data_path,config.data_file_list[config.data_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1M-eBfyUY0X"
   },
   "source": [
    "### 标签映射表构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "x_eRPXQPOy9_"
   },
   "outputs": [],
   "source": [
    "# 功能：标签映射表构建\n",
    "def build_label2id(data,data_path):\n",
    "  '''\n",
    "    功能：标签映射表构建\n",
    "    input:\n",
    "      data     DataFrame     训练数据\n",
    "    return:\n",
    "      label2id_dic Dict        label 到 id 的映射\n",
    "      id2label_dic Dict        id 到 label 的映射\n",
    "  '''\n",
    "  labeldata = data.groupby(['label', 'label_id']).count().reset_index()\n",
    "  label2id_dic = {}\n",
    "  id2label_dic = {}\n",
    "  for index,row in labeldata.T.iteritems():\n",
    "    label2id_dic[row['label']] = row['label_id']\n",
    "    id2label_dic[row['label_id']] = row['label']\n",
    "\n",
    "  label2id = {\n",
    "      \"label2id\":label2id_dic,\n",
    "      \"id2label\":id2label_dic\n",
    "  }\n",
    "\n",
    "  save_plk_dict(label2id,data_path,\"label2id\")\n",
    "  return label2id_dic,id2label_dic\n",
    "  \n",
    "label2id_dic,id2label_dic = build_label2id(train_data,config.data_path)\n",
    "train_data = train_data[['text','label_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9X-S4nRCYzvl"
   },
   "source": [
    "### 训练数据集拆分为训练集和验证集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dvXyvuNSYyjW"
   },
   "outputs": [],
   "source": [
    "train, valid = train_test_split(train_data, train_size=config.split_ratio, random_state=config.seed)\n",
    "if config.is_demo:\n",
    "  train = train[0:int(0.01*len(train))]\n",
    "  valid = train[0:int(0.01*len(valid))]\n",
    "train_labels = train.groupby(['label_id']).count().reset_index()\n",
    "valid_labels = valid.groupby(['label_id']).count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rKeHRxvgY3__"
   },
   "source": [
    "## bert 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "_ly378_dY5bN"
   },
   "outputs": [],
   "source": [
    "# 分词工具\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(config.bert_model_path, do_lower_case=config.do_lower_case)\n",
    "# 类初始化\n",
    "processor = DataPrecessForSingleSentence(bert_tokenizer= bert_tokenizer)\n",
    "# 加载预训练的bert模型\n",
    "model = BertForSequenceClassification.from_pretrained(config.bert_model_path, num_labels=config.num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phkXnzTYZBIv"
   },
   "source": [
    "## 数据编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "IyF-QDAG3mBa"
   },
   "outputs": [],
   "source": [
    "def build_data(processor,data,batch_size):\n",
    "  # 产生训练集输入数据\n",
    "  seqs, seq_masks, seq_segments, labels = processor.get_input(dataset=data)\n",
    "  # 转换为torch tensor\n",
    "  t_seqs = torch.tensor(seqs, dtype=torch.long)\n",
    "  t_seq_masks = torch.tensor(seq_masks, dtype = torch.long)\n",
    "  t_seq_segments = torch.tensor(seq_segments, dtype = torch.long)\n",
    "  t_labels = torch.tensor(labels, dtype = torch.long)\n",
    "\n",
    "  data = TensorDataset(t_seqs, t_seq_masks, t_seq_segments, t_labels)\n",
    "  sampler = RandomSampler(data)\n",
    "  dataloder = DataLoader(dataset= data, sampler=sampler, batch_size = batch_size)\n",
    "  return dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Due_5hwT4JBi"
   },
   "outputs": [],
   "source": [
    "train_dataloder = build_data(processor,train,config.batch_size)\n",
    "valid_dataloder = build_data(processor,valid,config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1DZVREvZUV_"
   },
   "source": [
    "## 模型参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "oPmFOXooZeBt"
   },
   "outputs": [],
   "source": [
    "# 待优化的参数\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params':[p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay':0.01},\n",
    "    {'params':[p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay':0.0}\n",
    "]\n",
    "\n",
    "steps = len(train_dataloder) * config.epochs\n",
    "optimizer = BertAdam(optimizer_grouped_parameters, lr=2e-05, warmup= 0.1 , t_total= steps)\n",
    "loss_function = LabelSmoothing(config.num_labels, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JTPmbdP2ZnoG"
   },
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "gdeKBfZrZsXF"
   },
   "outputs": [],
   "source": [
    "model = model.to(config.device)\n",
    "#存储loss\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "avg_train_losses = []\n",
    "avg_valid_losses = []\n",
    "early_stopping = EarlyStopping(patience=config.patience, verbose=True)\n",
    "best_f1 = 0\n",
    "for i in trange(config.epochs, desc='Epoch'): \n",
    "    model.train() #训练\n",
    "    for step, batch_data in enumerate(tqdm(train_dataloder, desc='Train Iteration')):\n",
    "        batch_data = tuple(t.to(config.device) for t in batch_data)\n",
    "        batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels = batch_data\n",
    "        # 对标签进行onehot编码\n",
    "        if torch.cuda.is_available():\n",
    "            one_hot = torch.zeros(batch_labels.size(0), config.num_labels).long().cuda()  #gpu版本\n",
    "            one_hot_batch_labels = one_hot.scatter_(\n",
    "                dim=1,\n",
    "                index=torch.unsqueeze(batch_labels, dim=1),\n",
    "                src=torch.ones(batch_labels.size(0), config.num_labels\n",
    "            ).long().cuda())\n",
    "        else:\n",
    "            # 以下注释为cpu版本\n",
    "            one_hot = torch.zeros(batch_labels.size(0), config.num_labels).long()   #cpu版本\n",
    "            one_hot_batch_labels = one_hot.scatter_(\n",
    "              dim=1,\n",
    "              index=torch.unsqueeze(batch_labels, dim=1),\n",
    "              src=torch.ones(batch_labels.size(0), config.num_labels\n",
    "            ).long())\n",
    "\n",
    "        logits = model(\n",
    "            batch_seqs, batch_seq_masks, batch_seq_segments, labels=None)\n",
    "        logits = torch.nn.functional.log_softmax(logits, dim=1)\n",
    "        #loss_function = CrossEntropyLoss()\n",
    "        loss = loss_function(logits, batch_labels)\n",
    "        loss.backward()\n",
    "        train_losses.append(loss.item())\n",
    "        print(\"\\r%f\" % loss, end='')\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    model.eval() #验证\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "    for step, batch_data in enumerate(\n",
    "            tqdm(valid_dataloder, desc='Dev Iteration')):\n",
    "        with torch.no_grad():\n",
    "            batch_data = tuple(t.to(config.device) for t in batch_data)\n",
    "            batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels = batch_data\n",
    "            if torch.cuda.is_available():\n",
    "                # 对标签进行onehot编码，以下注释为gpu版本\n",
    "                one_hot = torch.zeros(batch_labels.size(0), config.num_labels).long().cuda()\n",
    "                one_hot_batch_labels = one_hot.scatter_(\n",
    "                    dim=1,\n",
    "                    index=torch.unsqueeze(batch_labels, dim=1),\n",
    "                    src=torch.ones(batch_labels.size(0), config.num_labels\n",
    "                ).long().cuda())\n",
    "            else:\n",
    "                # cpu\n",
    "                one_hot = torch.zeros(batch_labels.size(0), config.num_labels).long()\n",
    "                one_hot_batch_labels = one_hot.scatter_(\n",
    "                  dim=1,\n",
    "                  index=torch.unsqueeze(batch_labels, dim=1),\n",
    "                  src=torch.ones(batch_labels.size(0), config.num_labels\n",
    "                ).long())\n",
    "\n",
    "            logits = model(\n",
    "                batch_seqs, batch_seq_masks, batch_seq_segments, labels=None)\n",
    "            logits = torch.nn.functional.log_softmax(logits, dim=1)\n",
    "            loss = loss_function(logits, batch_labels)\n",
    "            valid_losses.append(loss.item())\n",
    "            \n",
    "            logits = logits.softmax(dim=1).argmax(dim = 1)\n",
    "            pred_labels.append(logits.detach().cpu().numpy())\n",
    "            true_labels.append(batch_labels.detach().cpu().numpy())\n",
    "    \n",
    "    true_labels = np.concatenate(true_labels)\n",
    "    pred_labels = np.concatenate(pred_labels)\n",
    "    precision = precision_score(true_labels, pred_labels, average='micro')\n",
    "    recall = recall_score(true_labels, pred_labels, average='micro')\n",
    "    f1 = f1_score(true_labels, pred_labels, average='micro')\n",
    "    \n",
    "    if best_f1<f1:\n",
    "      # torch.save(model.state_dict(), 'checkpoint_f1.pt')\n",
    "      torch.save(model, open(\"checkpoint_f1.bin\", \"wb\"))\n",
    "\n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "    print(\"train_loss:%f, valid_loss:%f, precision:%f, recall:%f, f1:%f \" %(train_loss, valid_loss,precision,recall,f1))\n",
    "    \n",
    "    #重置训练损失和验证损失\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    early_stopping(valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early Stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzNElnGOcmqv"
   },
   "source": [
    "## 绘制loss图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "FWXXQQetcoWe"
   },
   "outputs": [],
   "source": [
    "# 功能：绘制 损失函数 loss 曲线\n",
    "def draw_loss_pic(avg_train_losses,avg_valid_losses):\n",
    "  '''\n",
    "    功能：绘制 损失函数 loss 曲线\n",
    "    input:\n",
    "      avg_train_losses\n",
    "      avg_valid_losses\n",
    "  '''\n",
    "  %matplotlib inline\n",
    "  fig = plt.figure(figsize=(8,6))\n",
    "  plt.plot(range(1, len(avg_train_losses)+1), avg_train_losses, label='Training Loss')\n",
    "  plt.plot(range(1, len(avg_valid_losses)+1), avg_valid_losses, label='Validation Loss')\n",
    "\n",
    "  #find the position of lowest validation loss\n",
    "  minposs = avg_valid_losses.index(min(avg_valid_losses))+1\n",
    "  plt.axvline(minposs, linestyle='--', color = 'r', label='Early Stopping Checkpoint')\n",
    "  plt.xlabel('epochs')\n",
    "  plt.ylabel('loss')\n",
    "  plt.grid(True)\n",
    "  plt.legend()\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "  fig.savefig('loss_plot.png', bbox_inches='tight')\n",
    "\n",
    "draw_loss_pic(avg_train_losses,avg_valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL8n0PwsyeZl"
   },
   "source": [
    "## 测试数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZpTv3A7-gVL"
   },
   "source": [
    "### 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "_FFb-85C-gxg"
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"checkpoint_f1.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlwAkbz4INE2"
   },
   "source": [
    "### 测试集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "bqmDvfgiJOSx"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "Ge9VQXKIICIr"
   },
   "outputs": [],
   "source": [
    "test_data = load_data(config.data_path,config.data_file_list[config.data_index])\n",
    "if config.is_demo:\n",
    "  test_data = test_data.sample(int(0.01*len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "epSkk_CxIWzc"
   },
   "outputs": [],
   "source": [
    "test_data = test_data[['text','label_id']]\n",
    "test_labels = test_data.groupby(['label_id']).count().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bCGQimE1IjiZ"
   },
   "source": [
    "### 测试数据构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "WSD-m_VdIclz"
   },
   "outputs": [],
   "source": [
    "test_dataloder = build_data(processor,test_data,config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "4Ag0bQWfItED"
   },
   "outputs": [],
   "source": [
    "# 用于存储预测标签与真实标签\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "model.eval()\n",
    "# 预测\n",
    "with torch.no_grad():\n",
    "    for batch_data in tqdm_notebook(test_dataloder, desc = 'TEST'):\n",
    "        batch_data = tuple(t.to(config.device) for t in batch_data)\n",
    "        batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels = batch_data        \n",
    "        logits = model(\n",
    "            batch_seqs, batch_seq_masks, batch_seq_segments, labels=None)\n",
    "        logits = logits.softmax(dim=1).argmax(dim = 1)\n",
    "        pred_labels.append(logits.detach().cpu().numpy())\n",
    "        true_labels.append(batch_labels.detach().cpu().numpy())\n",
    "# 查看各个类别的准确率和召回率\n",
    "result = classification_report(np.concatenate(true_labels), np.concatenate(pred_labels))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AA7OS6qeKfs1"
   },
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "sWpvoUVkIs7n"
   },
   "outputs": [],
   "source": [
    "query_list = [\n",
    "     \"25日股票基金全线受挫 九成半基金跌逾1%全景网3月26日讯 周三开放式基金净值普降，股票型基金全线受挫，九成半基金跌幅超过1%。上证综指昨日收市跌2.00%。据全景网基金统计数据，3月25日统计的229只股票型基金全线下挫，其中219只跌幅在1%以上，占比95.63%。跌幅排名前五位的基金是友邦红利ETF、富国天鼎、宝盈资源优选、德盛红利、易方达深100ETF，增长率分别为-3.03%、-2.70%、-2.51%、-2.44%、-2.42%。华富策略精选、景顺公司治理、长城双动力、荷银合丰稳定、汇丰晋信2026等跌幅较小，均在1%以内。积极配置型基金亦全盘尽墨，中欧新蓝筹、金鹰优选两只基金跌幅超过2%，天治财富增长、华夏回报跌幅在0.5%以内。保守配置型基金中，申万盛利配置最为抗跌，下挫0.20%，国投瑞银融华垫底，跌0.99%。债市方面，上证国债指数昨日涨0.03%，上证企债指数跌0.10%。普通债券型基金仅4只飘红，国投瑞银债券领跑，涨0.03%，中信稳定双利、易方达稳健收益B、易方达稳健收益A也小幅上扬。嘉实多元收益A、嘉实多元收益B、华富收益增强B、华富收益增强A跌幅都超过0.4%。（全景网/陈丹蓉）\",\n",
    "     \"古天乐投诉大S不给机会 希望下次再合作(组图)新浪娱乐讯 8月26日，电影《保持通话》举行宣传活动，众演员包括古天乐( 听歌 blog)、徐熙媛(大S)、张嘉辉及导演陈木胜等等均有出席。古天乐笑言这次拍摄多是单独演出，并要一直拿着手机演戏，感觉新鲜又甚具压力，他更表示与大S只有一场对手戏，希望下次能够再有机会合作。TUNGSTAR/文并图 \"\n",
    "]\n",
    "predict_data = pd.DataFrame([{'text':query, 'label_id':0} for query in query_list])\n",
    "predict_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "cp4ynXuWXUWA"
   },
   "outputs": [],
   "source": [
    "id2label_dic = load_plk_dict(config.data_path,\"label2id\")['id2label']\n",
    "id2label_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "RNhkA4jAMXhY"
   },
   "outputs": [],
   "source": [
    "predict_dataloder = build_data(processor,predict_data,config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "yGd3fwicKw39"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_labels = []\n",
    "# 预测\n",
    "with torch.no_grad():\n",
    "    for batch_data in tqdm_notebook(predict_dataloder, desc = 'Predict'):\n",
    "        batch_data = tuple(t.to(config.device) for t in batch_data)\n",
    "        batch_seqs, batch_seq_masks, batch_seq_segments, batch_labels = batch_data        \n",
    "        logits = model(\n",
    "            batch_seqs, batch_seq_masks, batch_seq_segments, labels=None)\n",
    "        logits = logits.softmax(dim=1).argmax(dim = 1)\n",
    "        pred_labels.append(logits.detach().cpu().numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "0c-JfpKyMq9F"
   },
   "outputs": [],
   "source": [
    "pred_labels"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "kXrO8vjIW1L0",
    "oTJKb1KmX73l",
    "NtBOnYqTYCiO",
    "phkXnzTYZBIv",
    "c1DZVREvZUV_",
    "JTPmbdP2ZnoG",
    "RzNElnGOcmqv",
    "cL8n0PwsyeZl"
   ],
   "name": "基于Bert做中文文本分类.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
